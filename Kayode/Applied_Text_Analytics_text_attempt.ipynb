{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read In Files  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#set working Directory to where class corpus is saved.\n",
    "\n",
    "#if using laptop\n",
    "#os.chdir('D:\\\\OneDrive - QJA\\\\My Files\\\\NW Coursework\\\\Predict 453 Natural Language Processesing\\\\Class Corpus')\n",
    "\n",
    "#if using workstation\n",
    "os.chdir('C:\\\\Users\\\\Jason\\\\OneDrive - QJA\\\\My Files\\\\NW Coursework\\\\Predict 453 Natural Language Processesing\\\\Class Corpus')\n",
    "\n",
    "data=pd.read_csv('Class Corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the following is an example where documents and the entire \n",
    "corpus is classified by gender.\n",
    "This is from textbook: Applied Text Analysis with Python: Bengfort et al\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull single article from the data object\n",
    "#text is in 'Text' column of 15th row\n",
    "\n",
    "text_single = data['Text'].apply(str)[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create single string of all the DSIs (each row of the text column)\n",
    "text_all = ' '.join(data['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male = 'male'\n",
    "female = 'female'\n",
    "unknown = 'unknown'\n",
    "both = 'both'\n",
    "\n",
    "male_words = set([\n",
    "    'guy', 'spokesman', 'chairman', \"men's\", 'men', 'him', \"he's\", 'his',\n",
    "    'boy', 'boyfriend', 'boyfriends', 'boys', 'brother', 'brothers', 'dad',\n",
    "    'dads', 'dude', 'duke', 'father', 'fathers', 'fiance', 'gentleman', 'gentlemen',\n",
    "    'god', 'grandfather', 'grandpa', 'grandson', 'groom', 'he', 'himself',\n",
    "    'husband', 'husbands', 'king', 'kings', 'male', 'man', 'mr', 'nephew',\n",
    "    'nephews', 'priest', 'son', 'sons', 'uncle', 'uncles', 'waiter', 'widower',\n",
    "    'widowers'\n",
    "])\n",
    "\n",
    "female_words = set([\n",
    "    'heroine', 'spokeswoman', 'chairwoman', \"women's\", 'women', 'actress', \"she's\",\n",
    "    'her', \"her's\", 'aunts', 'bride', 'daughter', 'daughters', 'female', 'fiancee',\n",
    "    'girl', 'girlfriend', 'girlfriends', 'girls', 'goddess', 'granddaughter', \n",
    "    'grandma', 'grandmother', 'herself', 'ladies', 'lady', 'mom', 'moms', 'mother',\n",
    "    'mothers', 'mrs', 'ms', 'niece', 'nieces', 'priestess', 'queens', 'she', \n",
    "    'sister', 'sisters', 'waitress', 'widow','widows', 'wife', 'wives', 'woman'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to assign gender to a sentence\n",
    "\n",
    "def genderize(words):\n",
    "    #counts number of times male and female words object \"words\" are found\n",
    "    #in the male_words list from above\n",
    "    mwlen = len(male_words.intersection(words))\n",
    "    fwlen = len(female_words.intersection(words))\n",
    "    \n",
    "    if mwlen > 0 and fwlen == 0:\n",
    "        return male\n",
    "    \n",
    "    elif mwlen == 0 and fwlen > 0:\n",
    "        return female\n",
    "    \n",
    "    elif mwlen > 0 and fwlen > 0:\n",
    "        return both\n",
    "    \n",
    "    else:\n",
    "        return unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to count freq of gendered words and sentences within text of an article\n",
    "#this function takes a list of sentences and applies genderzie function to eval\n",
    "#total number of gendered words and sentences\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def count_gender(sentences):\n",
    "    \n",
    "    sents = Counter()\n",
    "    words = Counter()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        gender = genderize(sentence)\n",
    "        #the += for text is like concatenating something to a list\n",
    "        sents[gender] += 1\n",
    "        words[gender] += len(sentence)\n",
    "    \n",
    "    return (sents, words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def parse_gender(text):\n",
    "    \n",
    "    sentences = [\n",
    "        [word.lower() for word in nltk.word_tokenize(sentence)]\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    \n",
    "    sents, words = count_gender(sentences)\n",
    "    total = sum(words.values())\n",
    "    \n",
    "    for gender, count in words.items():\n",
    "        pcent = (count / total) * 100\n",
    "        nsents = sents[gender]\n",
    "        \n",
    "        print(\n",
    "        \"{:0.1f}% {} ({} sentences)\".format(pcent, gender, nsents)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_gender(text_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_gender(text_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The code below is processing of the 453 SEC55 Trump Corpus\n",
    "Code below will be from:\n",
    "https://pynlp.wordpress.com/2014/02/03/7-corpus-processing/ \n",
    "http://www.nltk.org/book/ch01.html#sec-computing-with-language-simple-statistics\n",
    "Natural Language Processing text by Byrd et al\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this example, all the DSIs are in plain text in a folder called \"Donald_Trump\"\n",
    "#this will import the plain text reader from nltk to read all txt files in the folder\n",
    "\n",
    "#from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "nltk.download('punkt')\n",
    "\n",
    "#this tells reader to find .txt files and is called later when calling the reader\n",
    "DOC_PATTERN= r'(?!\\.)[\\w_\\s]+/[\\w\\s\\d\\-]+\\.txt'\n",
    "\n",
    "#this specifies the category patter (ie: if there were more than one folder or\n",
    "#category other than just Donald_Trump in this example)\n",
    "#use this if you are using CategoryPlaintTextCorpusReader\n",
    "#CAT_PATTERN =r'([\\w_\\s]+)/.*'\n",
    "\n",
    "#provide path to partent folder where each corpus folder is located\n",
    "#in this example, there is only one folder \"Donald_Trump\" so that's \n",
    "#all it will find.\n",
    "corpus = PlaintextCorpusReader(\n",
    "    '../Class Corpus/Corpus', DOC_PATTERN)\n",
    "\n",
    "#note the '../' tells it to start with where the python code is stored\n",
    "#so you don't have to write full path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads all the file names in the corpus you specified\n",
    "corpus_list = corpus.fileids()\n",
    "corpus_list\n",
    "\n",
    "#or to print:\n",
    "# for f in corpus.fileids():\n",
    "#     print (f)\n",
    "\n",
    "#this will index a specific corpus text\n",
    "#corpus.fileids()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want just the first part of a fileid name\n",
    "[fileid[:21] for fileid in corpus.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one method to pull a specific DSI from the corpus\n",
    "corpus.raw('Donald_Trump/AC_Doc1_Trump-Trade-S-Korea.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a specified list of characters from the raw corpus\n",
    "corpus.raw('Donald_Trump/PKC_Doc1_US-Canada-NAFTA.txt')[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through each document and print the first 10 words\n",
    "for id in corpus.fileids():\n",
    "    print(corpus.words(id)[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paras, sents, and words are functions specified in CategorizedPlaintextCorpusReader\n",
    "\n",
    "para_list = corpus.paras()\n",
    "sents_list = corpus.sents()\n",
    "word_list = corpus.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num paragraphs, num sentences, num words\n",
    "len(para_list), len(sents_list), len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_list_s = corpus.paras('Donald_Trump/AC_Doc1_Trump-Trade-S-Korea.txt')\n",
    "sents_list_s = corpus.sents('Donald_Trump/AC_Doc1_Trump-Trade-S-Korea.txt')\n",
    "word_list_s = corpus.words('Donald_Trump/AC_Doc1_Trump-Trade-S-Korea.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(para_list_s), len(sents_list_s), len(word_list_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileid in corpus.fileids():\n",
    "    num_chars = len(corpus.raw(fileid))\n",
    "    num_words = len(corpus.words(fileid))\n",
    "    num_sents = len(corpus.sents(fileid))\n",
    "    num_vocab = len(set([w.lower() for w in corpus.words(fileid)]))\n",
    "    \n",
    "    #print: total numb characters, total numb words, total numb sentences, total vocabulary\n",
    "    #print: av word length, av words per sentence, av numb times each vocab item appears in the text\n",
    "    print (int(num_chars), int(num_words), int(num_sents), int(num_vocab),\n",
    "           int(num_chars/num_words), int(num_words/num_sents), int(num_words/num_vocab),fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_ids = corpus.words(fileids=['Donald_Trump/PKC_Doc1_US-Canada-NAFTA.txt',\n",
    "                               'Donald_Trump/PKC_Doc1_US-Canada-NAFTA.txt'])\n",
    "two_ids, len(two_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "############## working with paragraphs, sentences, words, raw  ##############\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints every paragraph in the entire corpus\n",
    "for p in para_list:\n",
    "    print (p)\n",
    "    \n",
    "raw_input() #this waits to run until you hit enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints every sentence in the entire corpus\n",
    "for s in sents_list:\n",
    "    print (s)\n",
    "    \n",
    "raw_input()  #this waits to run until you hit enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print every word in the entire corpus\n",
    "for w in word_list:\n",
    "    print (w)\n",
    "    \n",
    "raw_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#produce the entire corpus\n",
    "corpus_raw = corpus.raw()\n",
    "\n",
    "#this will create a string with contents of entire corpus\n",
    "corpus_raw\n",
    "\n",
    "#this will print it and it will appear like it does in txt file\n",
    "#print (corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "######### operating on every element ############\n",
    "\n",
    "#only gets length for first 20 words\n",
    "#[len(w) for w in corpus.words('Donald_Trump/TAC_Doc2_Kavanaugh_Confirmation_Vote.txt')[0:20]]\n",
    "\n",
    "\"\"\"\n",
    "same as above but forces lower case for all words note that its important \n",
    "to use the one below because it eliminates python from double counting words \n",
    "due to capitalizaiton, ie: This and this are counted separately\n",
    "\"\"\"\n",
    "[len(w.lower()) for w in corpus.words('Donald_Trump/TAC_Doc2_Kavanaugh_Confirmation_Vote.txt')[0:20]]\n",
    "\n",
    "##references the object defined above for the specified document\n",
    "#[len(w) for w in word_list_s[0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example on the importance of using .lower referenced above\n",
    "\"\"\"\n",
    "NOTE: not applicable for PlaintextCorpusReader.  Use this to check\n",
    "if you are using not using NLTK corpus reader\n",
    "you will see that the length of the corpus is larger for the \n",
    "option where words aren't forced to lower case\n",
    "\"\"\"\n",
    "\n",
    "len(corpus.words()), len([w.lower() for w in corpus.words()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat example above but with sentences\n",
    "[len(s) for s in corpus.sents('Donald_Trump/TAC_Doc2_Kavanaugh_Confirmation_Vote.txt')[0:20]]\n",
    "\n",
    "##references the object defined above for the specified document\n",
    "#[len(s) for s in sent_list_s[0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "########## freq analaysis ##############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this shows how to get the length of each word in a document\n",
    "#its only for illustration since freqdist (shown later) does this more efficiently\n",
    "\n",
    "len_test = [len(w) for w in word_list]\n",
    "\n",
    "len_test2 = [len(w) for w in corpus.words('Donald_Trump/TAC_Doc2_Kavanaugh_Confirmation_Vote.txt')]\n",
    "\n",
    "len_test[0:10], len_test2[0:10]\n",
    "\n",
    "#note: the first two are the same because the first character measured is an apostrophe and\n",
    "#for these two documents, the next word is 'president' and 'kavanaugh' = both 9 letters\n",
    "#so just coincidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "#to get the freq of words in entire corpus (not lowercase)\n",
    "fdist_a = nltk.FreqDist(corpus.words())\n",
    "\n",
    "#to get the freq of a single text in a specific DSI (not lowercase)\n",
    "fdist_b = nltk.FreqDist(corpus.words('Donald_Trump/AC_Doc1_Trump-Trade-S-Korea.txt'))\n",
    "\n",
    "#Note: word_list is defined above\n",
    "fdist = nltk.FreqDist([w.lower() for w in word_list])\n",
    "\n",
    "#print(fdist) prints a summmary, while fdist prints the tuple with word and freq\n",
    "\n",
    "print(fdist), fdist\n",
    "\n",
    "#print(fdist_a), fdist_a\n",
    "#print(fdist_b), fdist_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get top n most common (or leave () blank and it defaults to 100 possibly?)\n",
    "fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the word that is most freq, get the freq of the most common word\n",
    "fdist.max(), fdist['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#print the 20 most common terms\n",
    "\"\"\"\n",
    "see that most are stop words (except for Trump) which is probably \n",
    "so common that its also not very useful\n",
    "\"\"\"\n",
    "fdist_plot = fdist.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing here but specifying specific words of interest\n",
    "\n",
    "#Note: word_list is defined above\n",
    "fdist_modal = nltk.FreqDist([w.lower() for w in word_list])\n",
    "modals = ['false', 'lie', 'accurate', 'falsifying', 'President', 'president']\n",
    "for m in modals:\n",
    "    print (m + ':', fdist[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way to do the above but using tokenize instead of corpusplaintextreader\n",
    "from nltk.probability import FreqDist\n",
    "words = nltk.tokenize.word_tokenize(corpus.raw())\n",
    "fdist = FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "###############  filter for words based on length #######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_words = [w for w in corpus.words('Donald_Trump/PKC_Doc1_US-Canada-NAFTA.txt') if len(w)>10]\n",
    "\n",
    "long_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find words in different documents that are based on specified criteria\n",
    "#see lines below for example on FreqDist\n",
    "\n",
    "txt1 = FreqDist(corpus.words('Donald_Trump/PKC_Doc1_US-Canada-NAFTA.txt'))\n",
    "sorted(w for w in corpus.words('Donald_Trump/PKC_Doc1_US-Canada-NAFTA.txt') \n",
    "       if len(w)>2 and txt1[w]>5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## find words based on conditions ########\n",
    "\n",
    "\"\"\"\n",
    "These are common word comparison operators:\n",
    "\n",
    "s.startswith(t) test if s starts with t\n",
    "s.endswith(t) test if s ends with t\n",
    "t in s test if t is a substring of s\n",
    "s.islower() test if s contains cased characters and all are lowercase\n",
    "s.isupper() test if s contains cased characters and all are uppercase\n",
    "s.isalpha() test if s is non-empty and all characters in s are alphabetic\n",
    "s.isalnum() test if s is non-empty and all characters in s are alphanumeric\n",
    "s.isdigit() test if s is non-empty and all characters in s are digits\n",
    "s.istitle() test if s contains cased characters and is titlecased \n",
    "                (i.e. all words in s have initial capitals)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(w for w in corpus.words('Donald_Trump/PKC_Doc1_US-Canada-NAFTA.txt')\n",
    "      if w.endswith('ing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use conditional or \n",
    "sorted(w for w in corpus.words()\n",
    "      if w.endswith('ible') or w.endswith('ious'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another option\n",
    "for suf in corpus.words():\n",
    "    if suf.endswith('ious'):\n",
    "        print(suf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tricky = sorted(w for w in corpus.words() if 'cie' in w or 'cei' in w)\n",
    "tricky\n",
    "\n",
    "# #option to print if you prefer\n",
    "# for word in tricky:\n",
    "#     print(word, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#\n",
    "#   Start Pre-Processing\n",
    "#\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "############# Use Dictionary to replace terms ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter EC dictionary for corpus DSIs to replace tokens\n",
    "#note: prob more efficient to read in txt file rather than paste is all here.\n",
    "\n",
    "Trump_dictionary = {\n",
    "\t'donaldtrump': ['president trump', 'president donald trump', 'donald trump'\n",
    "\t\t'mr trump', 'trumps', 'trump organization', 'donald j trump'],\n",
    "\t'trump administration': ['senior trump administration officials', 'the white house',\n",
    "\t\t'trump administration', 'trump administration officials', 'federal government', 'us official',\n",
    "\t\t'us officials', 'us government', 'donald trumps top economic advisors', 'white house sources',\n",
    "\t\t'state department official', 'the official', 'washington', 'washingtons', 'the administration',\n",
    "\t\t'white house source', 'administration officials', 'donald trumps top economic advisor', \n",
    "            'white house counsel', 'national security advisor', 'secretary of state', \n",
    "            'chairman of white house council of economic advisers', 'white house press secretary',\n",
    "            'press secretary', 'agriculture secretary'],\n",
    "\t'donaldmcgahan': ['donald f mcgahan II', 'don mcgahn', 'don f mcgahn', 'homeland security secretary'],\n",
    "\t'johnbolton': ['mr bolton', 'john bolton', 'bolton'],\n",
    "\t'mikepompeo': ['mike pompeo', 'pompeo'],\n",
    "\t'kirstjennielsen': ['kirstjen nielsen'],\n",
    "\t'kevenhassett': ['hassett'],\n",
    "\t'sarahsanders': ['sarah huckabee sanders', 'sanders'],\n",
    "\t'sonnyperdue': ['sonny perdue','mr perdue', 'mr perdues'],\n",
    "\t'mitchmcconnell': ['mitch mcconnell', 'mcconnell', 'senator mitch mcconnell',\n",
    "\t\t'majority leader', 'mr mcconnell'],\n",
    "\t'jeffflake': ['senator jeff flake', 'sen jeff flake'],\n",
    "\t'chuckgrassley': ['grassley', 'sen grassley', 'senator grassley'],\n",
    "\t'susancollins': ['sen susan collins', 'r-maine', 'senator collins', 'senator susan collins'],\n",
    "\t'berniesanders': ['sen bernie sanders', 'senator bernie sanders'],\n",
    "\t'chuckschumer': ['democratic majority leader', 'democratic majority leader chuck schumer'],\t\n",
    "\t'brettkavanaugh': ['kavanaugh', 'supreme court pick', 'supreme court nominee', 'judge brett m kavanaugh',\n",
    "\t\t'brett m kavanaugh', 'mr kavanaugh', 'president trumps nominee', 'nominee for the supreme court',\n",
    "\t\t'judge kavanaugh', 'supreme court nominee brett kavanaugh', 'brett', 'judge brett kavanaugh'],\n",
    "\t'brettkavanaughaccuser': ['deborah ramirez', 'christine blasey ford', 'charles ludington',\n",
    "\t\t'julie swetnick'],\n",
    "\t'brettkavanaughfriend': ['mark judge', 'pj smyth'],\n",
    "\t'christineblaseyford': ['dr ford', 'christine ford', 'ford'],\n",
    "\t'stephanieclifford': ['ex porn star stormy daniels', 'stormy daniels', 'clifford', 'ms daniels'],\n",
    "\t'michaelcohen': ['a lawyer for donald trump', 'cohen'],\n",
    "\t'melaniatrump': ['third wife melania', 'first lady', 'melania'],\n",
    "\t'georgewbush': ['george w bush', 'george bush'],\n",
    "\t'barackobama': ['obama administration', 'former president'],\n",
    "\t'chuckhagel': ['former defense secretary chuck hagel', 'hagel', \n",
    "                   'defense secretary for president barack obama'],\n",
    "\t'hillaryclinton': ['hillary', 'hillary clinton', 'hrc'],\n",
    "\t'johnpodestra': ['hillary clintons campaign chairman', 'hilaryclinton campaign chairman', 'podestra'],\n",
    "\t'julianassange': ['wikileaks founder julian assange', 'assange'],\n",
    "\t'robertmueller': ['mueller', 'special prosecutor'],\n",
    "\t'moonjae-in': ['moon jae-in', 'moon jaein', 'moon jae in' 'south korean president', 'moon'],\n",
    "\t'kimjong un': ['jong un'],\n",
    "\t'sebastianpinera': ['sebastian pinera'],\n",
    "\t'basharal-assad': ['assad', 'bashar al-assad',' bashar al assad', 'bashar alassad' 'al-assads',\n",
    "                      'al assads', 'alassads'],\n",
    "\t'vladimirputin': ['vladimir putin', 'putin'],\n",
    "\t'shinzoabe': ['prime minister of japan', 'abe', 'shinzo'],\n",
    "\t'xijinping': ['president of china', 'xi'],\n",
    "\t'andresmanuallopezobrador': ['andrew manual lopez obrandor', 'mexicos president elect', 'lopez obrador'],\n",
    "\t'justintrudeau': ['canadian prime minister', 'justin trudeau', 'mr trudeau'],\n",
    "\t'enriquepenanieto': ['enrique pena nieto'],\n",
    "\t'hassanrouhani': ['hassan rouhani', 'rouhani'],\n",
    "\t'senate judiciary': ['senate judiciary committee', 'judiciary committee'],\n",
    "\t'senate':['senator', 'united states senate', 'senate floor', 'senate majority leader','sen'],\n",
    "\t'congress': ['lawmakers', 'congressional'],\n",
    "\t'house of representatives': ['the house'],\n",
    "    'republican party': ['gop', 'conservatives', 'us republicans', 'republicans',\n",
    "\t\t'republican colleagues', 'republican'],\n",
    "\t'democratic party': ['democrat', 'us democrats', 'democrats'],\n",
    "\t'united states': ['us', 'u.s.', 'united states of america', 'american', 'americans',\n",
    "\t\t'americas'],\n",
    "\t'african americans': ['black workers'],\n",
    "\t'china': ['chinese', 'chinese government'],\n",
    "\t'syria': ['syrians'],\n",
    "\t'iran': ['iranians', 'iranian', 'tehran', 'basra', 'ayatollah'],\n",
    "\t'russia': ['russians', 'moscow', 'russian government', 'russian intelligence'],\n",
    "\t'european union': ['eu'],\n",
    "\t'canada': ['canadian'],\n",
    "\t'e3': ['european signatories', 'germany, france, and the united kingdom'],\n",
    "\t'opec': ['organization of the petroleum exporting countries', 'opec nations', \n",
    "\t\t'15-member Organization of the Petroleum Exporting Countries', 'opec source',\n",
    "\t\t'opec nations', 'mohammad sanusi barkindo, opecs sercurity general',\n",
    "\t\t'hossein kazempour ardebili irans govenor to opec'],\n",
    "\t'world trade organization': ['wto', 'world trade organization wto'],\n",
    "\t'council on foreign relations': ['lorand laskai'],\n",
    "\t'united nations': ['united nations general assembly'],\n",
    "\t'fbi': ['federal bureau of investigation', 'the bureau'],\n",
    "\t'department of health and human services': ['dhhs', 'hhs', 'health and human services',\n",
    "\t\t'hhss office of health reform'],\n",
    "\t'department of homeland security': ['dhs'],\n",
    "\t'deparment of justice': ['doj', 'justice department'],\n",
    "\t'bureau of labor and statistics': ['bls'],\n",
    "\t'bureau of economic analysis': ['bea'],\n",
    "\t'centers for medicare and medicaid services': ['cms'],\n",
    "\t'congressional budget office': ['cbo'],\n",
    "\t'islamic state': ['isis'],\n",
    "\t'us military': ['us forces', 'pentagon', 'us troops'],\n",
    "\t'us media': ['white house pool', 'reporters', 'nbc news', 'washington post', 'the post',\n",
    "\t\t'defense one', 'cnn', 'cnns anderson cooper', 'cnns tal kopan', 'cnns john defterios',\n",
    "\t\t'media outlets', 'abc', 'abcs four corners' 'bloomberg news', 'american media', \n",
    "\t\t'wall street journal', 'abs good morning america', 'access hollywood', 'dailymail.com',\n",
    "\t\t'cnbc', 'businessinsider.com', 'gallup', 'gallup panel', 'reuters'],\n",
    "\t'russian media': ['rt', 'sputnik'],\n",
    "\t'chinese media': ['chinese-language newspaper'],\n",
    "\t'undocumented immigrants': ['undocumented migrants', 'immigrants'],\n",
    "\t'immigrant separation': ['separation of parents and children', 'taken from parents',\n",
    "\t\t'separations', 'split families', 'reunite children', 'reunite families', \n",
    "\t\t'reunite families already separated', 'reunite kids', 'separated families',\n",
    "\t\t'family separation', 'away from their kids', 'separate families'],\n",
    "\t'immigrant detention': ['prosecuting undocumented migrants', 'children being held in jail', \n",
    "\t\t'locking up of entire families', 'detain families', 'detail those families', 'prosecute adults',\n",
    "\t\t'families will be detained', 'detained families', 'indefinite detention'],\n",
    "\t'united states-korea free trade agreement': ['korus'],\n",
    "\t'north american free trade agreement': ['nafta', 'nafta deal'],\n",
    "\t'trade war': ['us tarrif', 'us tarrifs', 'trade standoff'],\n",
    "\t'sanctions': ['sanctions', 'economic sanctions', 'us sanctions'],\n",
    "\t'economy': ['market cycle'],\n",
    "\t'capital expenditure': ['capex', 'capital investment'],\n",
    "\t'economic growth': ['recovery', 'cyclical upswing'],\n",
    "\t'economic suppression': ['financial crisis', 'downturn', 'recession', 'market disruption'\n",
    "\t\t'weaker growth'],\n",
    "\t'gross domestic product': ['gdp', 'gdp rate', 'gdp growth'],\n",
    "\t'unemployment': ['unemployment rate', 'manufacturing jobs', 'jobless numbers',\n",
    "\t\t'unemployment level' 'jobless rate', 'job creation', 'new jobs'],\n",
    "\t'stock market': ['s&p 500', 's&p 500 index', 'sp 500', 'sp 500 index', 's p 500', 's p 500 index'],\n",
    "\t'affordable care act': ['aca', 'aca plan', 'aca plans', 'obamacare', 'aca regulations',\n",
    "\t\t'acas regulations', 'aca health plans'],\n",
    "\t'short term insurance': ['short term plans', 'short term health plans', 'short-term plans',\n",
    "\t\t'short term medical plans', 'junk insurance' 'short term policies', 'skimpy plans', \n",
    "\t\t'skimpy health plans', 'short term limited duration plans', 'bare bones plans'],\n",
    "\t'financial institution': ['credit suisse', 'july beige book', 'federal reserve bank',\n",
    "\t\t'business roundtable', 'beige book'],\n",
    "\t'chief financial officer': ['cfo', 'cfos', 'chief financial officers', 'global cfo council'],\n",
    "\t'twitter': ['tweet', 'tweeted', 'tweeting'],\n",
    "\t'inaccuracy': ['erred', 'wrong', 'inconsistency', 'false tweet', 'error', 'misquoted',\n",
    "\t\t'false'\t'not even close to accurate', 'falsifying'],\n",
    "\t'data': ['historical data', 'benchmark', 'deletes and reposts', 'government data',\n",
    "\t\t'statistics', 'economic data', 'survey']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test to make sure it loaded\n",
    "Trump_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out which variables are which for the function to be defined below\n",
    "for k, v in Trump_dictionary.items():\n",
    "    #print (k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through and use dictionary to create equivalency terms\n",
    "#note: k is the key (ie: the key elements in dictionary and \n",
    "#v is the dictionary term for each key term)\n",
    "\n",
    "### THIS DOES NOT WORK  it replaces zero words\n",
    "\n",
    "def equivalence_class(dictionary, text):\n",
    "    for k, v in sorted(dictionary.items(),reverse=True):\n",
    "        for i in dictionary[k]:\n",
    "            text = re.sub(r\"\\b%s\\b\" % i, k, text)\n",
    "        ec_processed_text.append(text)\n",
    "    return text\n",
    "\n",
    "equivalence_class(Trump_dictionary, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "############ Test Stemmer Results  ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "########## Test Lemetization Results #################\n",
    "#compare to stemmed results\n",
    "#consider modeling both and see if one produces better clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "##################  Remove stopwords  ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "############# Tage Words before Bi/Tri grams ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "all_words=nltk.FreqDist(w.lower() for w in corpus.words() \n",
    "                        if w.lower() not in nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot freq distribution after removing stopwords\n",
    "fdist_plot = all_words.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "###########  bigrams  ###############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = corpus.words('Donald_Trump/PKC_Doc1_US-Canada-NAFTA.txt')\n",
    "\n",
    "#note: if you don't use list as a wrapper, ie: only use: nltk.bigrams(bigrams), you get \n",
    "#<generator object bigrams at 0x00000204DC17A830> telling you its ready to process\n",
    "#you just need to use list() to compute\n",
    "#specified to only get the first 30 bigrams of the text\n",
    "\n",
    "list(nltk.bigrams(bigrams))[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "###########  tri grams  ###############\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "###########  n-grams  ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "###########  tri grams  ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#\n",
    "#   Start Modeling  (see Paul's code)\n",
    "#\n",
    "#################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
