{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocol Extraction - Dictionary for Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import spacy\n",
    "import mglearn\n",
    "\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.decomposition import  LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a consolidate pool of  protocol extractions\n",
    "\n",
    "#### - Should this be general or by indication?\n",
    "#### - will we have enough data to build dictionary by indication?\n",
    "#### - Are there other options or supplements?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE1.csv\n",
      "PE10.csv\n",
      "PE11.csv\n",
      "PE12.csv\n",
      "PE13.csv\n",
      "PE14.csv\n",
      "PE15.csv\n",
      "PE16.csv\n",
      "PE17.csv\n",
      "PE18.csv\n",
      "PE19.csv\n",
      "PE2.csv\n",
      "PE21.csv\n",
      "PE22.csv\n",
      "PE23.csv\n",
      "PE24.csv\n",
      "PE25.csv\n",
      "PE26.csv\n",
      "PE27.csv\n",
      "PE28.csv\n",
      "PE3.csv\n",
      "PE4.csv\n",
      "PE5.csv\n",
      "PE6.csv\n",
      "PE7.csv\n",
      "PE8.csv\n",
      "PE9.csv\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "\n",
    "df = pd.DataFrame(columns = ['extract', 'studyid'])\n",
    "\n",
    "list_of_files = glob.glob('C:/Users/AyankoyaKayode/OneDrive - PRA Health Sciences/PRA/NLP/protocols/All/Extractions/*.csv')\n",
    "for file_name in list_of_files:\n",
    "    dp = pd.read_csv(file_name, header = None, encoding = \"ISO-8859-1\", names=['extract']) \n",
    "    dp['studyid'] = os.path.basename(file_name)\n",
    "    df = pd.concat([df, dp], ignore_index=True)\n",
    "    print(os.path.basename(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(df)\n",
    "n_features =  round(n_samples)\n",
    "n_topics = len(df.groupby('studyid')['studyid'].count())\n",
    "n_top_words = 20\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    list_term_temp=[]\n",
    "    #list_idx=[]\n",
    "\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        #list_term_temp=[]\n",
    "        #print(\"Topic #%d:\" % topic_idx)\n",
    "        #list_idx.append(topic_idx)\n",
    "        #print(\" \".join([feature_names[i]\n",
    "        #                for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        for i in topic.argsort()[:-n_top_words -1:-1]:\n",
    "            list_term_temp.append(feature_names[i])\n",
    "            \n",
    "        #list_term.append(list_term_temp)\n",
    "    \n",
    "    #dic=pd.DataFrame({'topic_index':list_idx, 'terms':list_term})\n",
    "    #print()\n",
    "    return list_term_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2580 items in the dictionary\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} items in the dictionary\".format(n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionaries of unigrams (single words), bigrams (2 words) and trigrams (3 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data_non=df.dropna()\n",
    "\n",
    "incld_list=whole_data_non['extract'].tolist()\n",
    "\n",
    "\n",
    "data_samples_incld = incld_list\n",
    "\n",
    "#### Unigram\n",
    "tf_vectorizer_incld1 = CountVectorizer(max_df=0.6, min_df=5, ngram_range=(1,1),\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf_incld1 = tf_vectorizer_incld1.fit_transform(data_samples_incld)\n",
    "\n",
    "#### bigrams\n",
    "tf_vectorizer_incld2 = CountVectorizer(max_df=0.6, min_df=4, ngram_range=(2,2),\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf_incld2 = tf_vectorizer_incld2.fit_transform(data_samples_incld)\n",
    "\n",
    "#### trigram\n",
    "\n",
    "tf_vectorizer_incld3 = CountVectorizer(max_df=0.6, min_df=4, ngram_range=(3,3),\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf_incld3 = tf_vectorizer_incld3.fit_transform(data_samples_incld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>022</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>120</th>\n",
       "      <th>13</th>\n",
       "      <th>13c</th>\n",
       "      <th>...</th>\n",
       "      <th>withdraws</th>\n",
       "      <th>wk</th>\n",
       "      <th>wocbps</th>\n",
       "      <th>women</th>\n",
       "      <th>worsening</th>\n",
       "      <th>written</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1338 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  022  10  100  1000  11  12  120  13  13c  ...   withdraws  wk  wocbps  \\\n",
       "0    0    0   0    0     0   0   0    0   0    0  ...           0   0       0   \n",
       "1    0    0   0    0     0   0   0    0   0    0  ...           0   0       0   \n",
       "2    0    0   0    0     0   0   0    0   0    0  ...           0   0       0   \n",
       "3    0    0   0    0     0   0   0    0   0    0  ...           0   0       0   \n",
       "4    0    0   0    0     0   0   0    0   0    0  ...           0   0       0   \n",
       "\n",
       "   women  worsening  written  year  years  yes  york  \n",
       "0      0          0        0     0      0    0     0  \n",
       "1      0          0        0     0      0    0     0  \n",
       "2      0          0        0     0      0    0     0  \n",
       "3      0          0        0     0      0    0     0  \n",
       "4      0          0        0     0      0    0     0  \n",
       "\n",
       "[5 rows x 1338 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tf_incld1.toarray(),columns=tf_vectorizer_incld1.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000 cells</th>\n",
       "      <th>000 mm3</th>\n",
       "      <th>022 assessed</th>\n",
       "      <th>10 day</th>\n",
       "      <th>10 mg</th>\n",
       "      <th>10 subject</th>\n",
       "      <th>10 subjects</th>\n",
       "      <th>100 000</th>\n",
       "      <th>100 response</th>\n",
       "      <th>11 subject</th>\n",
       "      <th>...</th>\n",
       "      <th>written informed</th>\n",
       "      <th>year screening</th>\n",
       "      <th>years accordance</th>\n",
       "      <th>years age</th>\n",
       "      <th>years older</th>\n",
       "      <th>years prior</th>\n",
       "      <th>years screening</th>\n",
       "      <th>years treatment</th>\n",
       "      <th>york heart</th>\n",
       "      <th>zileuton medication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1648 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000 cells  000 mm3  022 assessed  10 day  10 mg  10 subject  10 subjects  \\\n",
       "0          0        0             0       0      0           0            0   \n",
       "1          0        0             0       0      0           0            0   \n",
       "2          0        0             0       0      0           0            0   \n",
       "3          0        0             0       0      0           0            0   \n",
       "4          0        0             0       0      0           0            0   \n",
       "\n",
       "   100 000  100 response  11 subject         ...           written informed  \\\n",
       "0        0             0           0         ...                          0   \n",
       "1        0             0           0         ...                          0   \n",
       "2        0             0           0         ...                          0   \n",
       "3        0             0           0         ...                          0   \n",
       "4        0             0           0         ...                          0   \n",
       "\n",
       "   year screening  years accordance  years age  years older  years prior  \\\n",
       "0               0                 0          0            0            0   \n",
       "1               0                 0          0            0            0   \n",
       "2               0                 0          0            0            0   \n",
       "3               0                 0          0            0            0   \n",
       "4               0                 0          0            0            0   \n",
       "\n",
       "   years screening  years treatment  york heart  zileuton medication  \n",
       "0                0                0           0                    0  \n",
       "1                0                0           0                    0  \n",
       "2                0                0           0                    0  \n",
       "3                0                0           0                    0  \n",
       "4                0                0           0                    0  \n",
       "\n",
       "[5 rows x 1648 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tf_incld2.toarray(),columns=tf_vectorizer_incld2.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>022 assessed patients</th>\n",
       "      <th>100 000 cells</th>\n",
       "      <th>100 response headache</th>\n",
       "      <th>12 lead ecg</th>\n",
       "      <th>12 lead ecgs</th>\n",
       "      <th>12 months prior</th>\n",
       "      <th>12 visit week</th>\n",
       "      <th>12 week period</th>\n",
       "      <th>120 days dose</th>\n",
       "      <th>13 16 19</th>\n",
       "      <th>...</th>\n",
       "      <th>withdrawal criteria subject</th>\n",
       "      <th>withdrawal spermicides lactational</th>\n",
       "      <th>withdrawal subject fails</th>\n",
       "      <th>women childbearing potential</th>\n",
       "      <th>written informed consent</th>\n",
       "      <th>years accordance schedule</th>\n",
       "      <th>years age older</th>\n",
       "      <th>years prior screening</th>\n",
       "      <th>york heart association</th>\n",
       "      <th>zileuton medication interfere</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 961 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   022 assessed patients  100 000 cells  100 response headache  12 lead ecg  \\\n",
       "0                      0              0                      0            0   \n",
       "1                      0              0                      0            0   \n",
       "2                      0              0                      0            0   \n",
       "3                      0              0                      0            0   \n",
       "4                      0              0                      0            0   \n",
       "\n",
       "   12 lead ecgs  12 months prior  12 visit week  12 week period  \\\n",
       "0             0                0              0               0   \n",
       "1             0                0              0               0   \n",
       "2             0                0              0               0   \n",
       "3             0                0              0               0   \n",
       "4             0                0              0               0   \n",
       "\n",
       "   120 days dose  13 16 19              ...                \\\n",
       "0              0         0              ...                 \n",
       "1              0         0              ...                 \n",
       "2              0         0              ...                 \n",
       "3              0         0              ...                 \n",
       "4              0         0              ...                 \n",
       "\n",
       "   withdrawal criteria subject  withdrawal spermicides lactational  \\\n",
       "0                            0                                   0   \n",
       "1                            0                                   0   \n",
       "2                            0                                   0   \n",
       "3                            0                                   0   \n",
       "4                            0                                   0   \n",
       "\n",
       "   withdrawal subject fails  women childbearing potential  \\\n",
       "0                         0                             0   \n",
       "1                         0                             0   \n",
       "2                         0                             0   \n",
       "3                         0                             0   \n",
       "4                         0                             0   \n",
       "\n",
       "   written informed consent  years accordance schedule  years age older  \\\n",
       "0                         0                          0                0   \n",
       "1                         0                          0                0   \n",
       "2                         0                          0                0   \n",
       "3                         0                          0                0   \n",
       "4                         0                          0                0   \n",
       "\n",
       "   years prior screening  york heart association  \\\n",
       "0                      0                       0   \n",
       "1                      0                       0   \n",
       "2                      0                       0   \n",
       "3                      0                       0   \n",
       "4                      0                       0   \n",
       "\n",
       "   zileuton medication interfere  \n",
       "0                              0  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "\n",
       "[5 rows x 961 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tf_incld3.toarray(),columns=tf_vectorizer_incld3.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=2580 and n_features=2580...\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "\n",
    "lda_incld1 = lda.fit(tf_incld1)\n",
    "lda_incld2 = lda.fit(tf_incld2)\n",
    "lda_incld3 = lda.fit(tf_incld3)\n",
    "\n",
    "tf_feature_names_incld1 = tf_vectorizer_incld1.get_feature_names()\n",
    "incld_terms1 = print_top_words(lda_incld1, tf_feature_names_incld1, n_top_words)\n",
    "\n",
    "tf_feature_names_incld2 = tf_vectorizer_incld2.get_feature_names()\n",
    "incld_terms2 = print_top_words(lda_incld2, tf_feature_names_incld2, n_top_words)\n",
    "\n",
    "tf_feature_names_incld3 = tf_vectorizer_incld3.get_feature_names()\n",
    "incld_terms3 = print_top_words(lda_incld3, tf_feature_names_incld3, n_top_words)\n",
    "\n",
    "incld_terms_list1 = list(dict(Counter(incld_terms1).most_common(100)))\n",
    "incld_terms_list2 = list(dict(Counter(incld_terms2).most_common(100)))\n",
    "incld_terms_list3 = list(dict(Counter(incld_terms3).most_common(100)))\n",
    "\n",
    "unique_incld1 = list(set(incld_terms_list1))\n",
    "#unique_excld = list(set(excld_terms_list) - set(incld_terms_list)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       \n",
      "--------      \n",
      "diabetes      \n",
      "addition      \n",
      "efficacy      \n",
      "75            \n",
      "cycle         \n",
      "approximately \n",
      "diagnosed     \n",
      "platelets     \n",
      "phone         \n",
      "planned       \n",
      "\n",
      "\n",
      "topic 1       \n",
      "--------      \n",
      "ecgs          \n",
      "40            \n",
      "cirrhosis     \n",
      "central       \n",
      "classification\n",
      "eosinophilia  \n",
      "fluid         \n",
      "make          \n",
      "demonstrate   \n",
      "exception     \n",
      "\n",
      "\n",
      "topic 2       \n",
      "--------      \n",
      "hads          \n",
      "laboratory    \n",
      "explanation   \n",
      "positive      \n",
      "medical       \n",
      "month         \n",
      "consent       \n",
      "institutional \n",
      "monitored     \n",
      "includes      \n",
      "\n",
      "\n",
      "topic 3       \n",
      "--------      \n",
      "medicinal     \n",
      "comply        \n",
      "exploratory   \n",
      "delayed       \n",
      "efficacy      \n",
      "currently     \n",
      "porocess      \n",
      "lcm           \n",
      "plan          \n",
      "23            \n",
      "\n",
      "\n",
      "topic 4       \n",
      "--------      \n",
      "infectious    \n",
      "manner        \n",
      "low           \n",
      "permanent     \n",
      "lasting       \n",
      "local         \n",
      "intolerable   \n",
      "lives         \n",
      "exclude       \n",
      "moderate      \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting=np.argsort(lda.components_)[:,::-1]\n",
    "features=np.array(tf_vectorizer_incld1.get_feature_names())\n",
    "\n",
    "import mglearn\n",
    "dd1 = mglearn.tools.print_topics(topics=range(5), feature_names=features,sorting=sorting, topics_per_chunk=1, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'combination tsr': 3,\n",
       " 'days myoclonic': 2,\n",
       " 'montelukast norfloxacin': 2,\n",
       " '30 days': 2,\n",
       " 'independent ethics': 2,\n",
       " '11 subject': 2,\n",
       " 'inhibitor plus': 2,\n",
       " 'insulin use': 2,\n",
       " 'anti insulin': 2,\n",
       " 'approved written': 2,\n",
       " '14 subject': 2,\n",
       " '11 subjects': 2,\n",
       " 'drug level': 2,\n",
       " 'inconsistencies conmed': 2,\n",
       " 'following criteria': 2,\n",
       " 'aes special': 2,\n",
       " 'confirmed aia': 2,\n",
       " 'continue study': 2,\n",
       " 'dose imp': 2,\n",
       " 'condition opinion': 2,\n",
       " 'confirmed active': 2,\n",
       " '72 hours': 2,\n",
       " 'entire study': 2,\n",
       " '400mg day': 2,\n",
       " 'alkaline phosphatase': 2,\n",
       " 'increase 25': 2,\n",
       " 'blind maintenance': 2,\n",
       " 'clinic visit': 2,\n",
       " 'consecutive days': 2,\n",
       " 'inclusion subjects': 2,\n",
       " 'combination chemotherapy': 2,\n",
       " 'criteria subjects': 1,\n",
       " '986036 administration': 1,\n",
       " '3rd degree': 1,\n",
       " 'considered adverse': 1,\n",
       " 'alectinib ceritinib': 1,\n",
       " 'criterion 11': 1,\n",
       " 'mg prednisone': 1,\n",
       " 'monitor consulted': 1,\n",
       " '16 19': 1,\n",
       " 'irecist icpd': 1,\n",
       " 'et visit': 1,\n",
       " 'cycle cycle': 1,\n",
       " 'days preceding': 1,\n",
       " 'conmed reporting': 1,\n",
       " 'cord compression': 1,\n",
       " '12 visit': 1,\n",
       " 'dose escalation': 1,\n",
       " 'days compared': 1,\n",
       " '240 mg': 1,\n",
       " 'case report': 1,\n",
       " 'blood sample': 1,\n",
       " 'cda note': 1,\n",
       " 'depression anxiety': 1,\n",
       " 'dose whichever': 1,\n",
       " 'inadequate response': 1,\n",
       " 'count anc': 1,\n",
       " 'discontinued study': 1,\n",
       " 'days end': 1,\n",
       " 'lead ecg': 1,\n",
       " 'conditions inability': 1,\n",
       " 'eastern cooperative': 1,\n",
       " 'dwc subject': 1,\n",
       " 'dyslipidemic medications': 1,\n",
       " 'clinical laboratory': 1,\n",
       " 'ciprofloxacin daidzein': 1,\n",
       " 'achieve primary': 1,\n",
       " 'achenbach cbcl': 1,\n",
       " 'effect sustained': 1,\n",
       " 'hematopoietic growth': 1,\n",
       " 'does meet': 1,\n",
       " 'mtd obd': 1,\n",
       " 'infection requiring': 1,\n",
       " 'ip immediately': 1,\n",
       " 'compared baseline': 1,\n",
       " 'following measures': 1,\n",
       " 'investigator sponsor': 1,\n",
       " 'exclusion 16': 1,\n",
       " 'ability participate': 1,\n",
       " 'continue participant': 1,\n",
       " 'dostarlimab combination': 1,\n",
       " 'events adverse': 1,\n",
       " 'assessments completed': 1,\n",
       " 'acyclovir allopurinol': 1,\n",
       " 'immediately stop': 1,\n",
       " 'compared prospective': 1,\n",
       " 'ip non': 1,\n",
       " 'addition study': 1,\n",
       " 'inflammatory bowel': 1,\n",
       " 'clinical benefit': 1,\n",
       " 'dosage allowed': 1,\n",
       " 'corticosteroids stable': 1,\n",
       " 'consent withdrawn': 1,\n",
       " 'mrd negativity': 1,\n",
       " 'high dose': 1,\n",
       " 'modification diet': 1,\n",
       " '15 mg': 1,\n",
       " 'hormonal therapy': 1,\n",
       " 'age sex': 1,\n",
       " 'current local': 1}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(Counter(incld_terms2).most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dose study treatment': 3,\n",
       " 'impact bms 986036': 2,\n",
       " 'week 52 ptfu': 2,\n",
       " '52 ptfu visit': 2,\n",
       " 'safety tolerability endpoint': 2,\n",
       " '13 16 19': 2,\n",
       " 'secondary safety tolerability': 2,\n",
       " 'seizures 28 days': 2,\n",
       " 'chemotherapy treatment period': 2,\n",
       " 'clinically relevant increase': 2,\n",
       " '24 hours prior': 2,\n",
       " '13c methacetin breath': 2,\n",
       " 'methacetin breath test': 2,\n",
       " 'safety endpoint changes': 2,\n",
       " 'period video eeg': 2,\n",
       " 'baseline period video': 2,\n",
       " 'end maintenance period': 2,\n",
       " 'endpoint proportion subjects': 2,\n",
       " 'maintenance period video': 2,\n",
       " 'eeg end maintenance': 2}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(Counter(incld_terms3).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting=np.argsort(lda.components_)[:,::-1]\n",
    "features=np.array(tf_vectorizer_incld.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       \n",
      "--------      \n",
      "video eeg     \n",
      "period video  \n",
      "partial onset \n",
      "maintenance period\n",
      "onset seizures\n",
      "end maintenance\n",
      "mg qd         \n",
      "safety analysis\n",
      "end baseline  \n",
      "dose level    \n",
      "\n",
      "\n",
      "topic 1       \n",
      "--------      \n",
      "clinically significant\n",
      "study medication\n",
      "12 lead       \n",
      "safety tolerability\n",
      "active tb     \n",
      "secondary safety\n",
      "immunodeficiency virus\n",
      "human immunodeficiency\n",
      "including limited\n",
      "exclusion criterion\n",
      "\n",
      "\n",
      "topic 2       \n",
      "--------      \n",
      "exclusion criteria\n",
      "following criteria\n",
      "meet following\n",
      "mbt substudy  \n",
      "breath test   \n",
      "methacetin breath\n",
      "13c methacetin\n",
      "inclusion exclusion\n",
      "study inclusion\n",
      "addition study\n",
      "\n",
      "\n",
      "topic 3       \n",
      "--------      \n",
      "change baseline\n",
      "secondary endpoint\n",
      "entry value   \n",
      "week period   \n",
      "run period    \n",
      "12 week       \n",
      "28 day        \n",
      "day run       \n",
      "exploratory endpoint\n",
      "baseline 28   \n",
      "\n",
      "\n",
      "topic 4       \n",
      "--------      \n",
      "alt ast       \n",
      "total bilirubin\n",
      "study drug    \n",
      "gilbert syndrome\n",
      "upper limit   \n",
      "limit normal  \n",
      "drug induced  \n",
      "ast alp       \n",
      "normal uln    \n",
      "liver injury  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "dd1 = mglearn.tools.print_topics(topics=range(5), feature_names=features,sorting=sorting, topics_per_chunk=1, n_words=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PDFMIner - https://www.binpress.com/manipulate-pdf-python/\n",
    "\n",
    "BeautifulSoup - https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "\n",
    "PyTextRank - https://medium.com/@aneesha/beyond-bag-of-words-using-pytextrank-to-find-phrases-and-summarize-text-f736fa3773c5\n",
    "\n",
    "Text Summarization with NLTK in Python - https://stackabuse.com/text-summarization-with-nltk-in-python/\n",
    "\n",
    "Text summarization in 5 steps using NLTK - https://becominghuman.ai/text-summarization-in-5-steps-using-nltk-65b21e352b65\n",
    "\n",
    "TFIDF - https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8\n",
    "\n",
    "NLP For Topic Modeling Summarization Of Financial Documents  https://blog.usejournal.com/nlp-for-topic-modeling-summarization-of-financial-documents-10-k-q-93070db96c1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice subject to play with LDA on! It might also be cool to see how treating individual sentences as documents could affect topics. Computationally more expensive, but it might be feasible.\n",
    "\n",
    "https://towardsdatascience.com/basic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
